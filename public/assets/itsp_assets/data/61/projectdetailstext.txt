<h3 style="text-align: justify;">Flowchart describing the working of our project:</h3><p> <img src="https://scontent-sin1-1.xx.fbcdn.net/hphotos-xpf1/v/t1.0-9/10304337_412418835596821_1375479379654796423_n.jpg?oh=d0358136fa6fc30fbd1e1ee54a7bb16b&amp;oe=55FAFA9B"></p><p><strong>      </strong></p><p style="text-align: justify;"><strong>User Instructions :                                                                                            </strong></p><p><strong>                                                                                                                                                                                 </strong></p><ul><li>First the user switches ON the circuit board containing the Bluetooth Module and ATMega2560. </li><li>Next the user enables the Bluetooth of the phone and opens the App. The change in the blinking frequency of the Bluetooth Module to once in 2 seconds will confirm pairing.(If the bluetooth is not enabled properly, the user will prompt the user to switch it ON.) </li><li>The user should then start making gestures one by one. The recognised gesture is sent to the App once in every 3 seconds. So the approximate interval for which the user should make  a particular gesture is 3 seconds. </li><li>There are two Text Boxes in our App. </li><li>Once the user has finished his gestures, the user clicks the Speak button, and the text after Spell Check , addition of Spaces and automatic correction is displayed  in the Formatted Text field and converted to Speech.</li></ul><p style="text-align: justify;"><strong>Technical Details:</strong></p><ul>
<li><strong style="text-align: justify;"></strong>As the app is switched ON, it sends a pairing request to the bluetooth module and gets paired if a match is found. The maximum time in which a single gesture must be made is 3 seconds. A gesture gets recognised in this interval and is sent to the App via bluetooth. </li><li>The flex sensor and contact sensor readings are taken at regular intervals of 50 milliseconds starting from 0 whereas the accelerometer readings are taken at regular intervals of the same interval but starting at 50 milliseconds. All the Analog readings obtained (Flex sensors and accelerometer)  are converted to digital values and stored in arrays and structures.</li><li>The templates of minimum and maximum readings of the flex sensors and the readings of the contact sensors for a given alphabet are tabulated and stored in the AVR code. Each time a reading is taken, a check is performed as to which alphabet the reading corresponds to. The alphabet which is obtained the maximum number of times during thee checks in the 3 second interval is set to be the letter corresponding to the input.  </li><li>The letter set to correspond to the given input is sent through the bluetooth to the App at the end of the 3 second interval. If the gesture doesn't correspond to any letter, then no data is sent tot he App. </li><li>Letters 'i' and 'j' have the same flex and contact sensor readings, and are distinguished by motion i.e. accelerometer readings. So, in case i or j match with the input, the code sends the accelerometer readings obtained during the given input to the App. </li><li>The App code has pre-entered templates corresponding to accelerometer readings of 'i' and 'j'. The accelerometer readings are sent in a protocol defined by us. The App figures out that the readings obtained are accelerometer readings and finds out the similarity to the template by using the process of <em>Dynamic Time Warping, </em>a standard algorithm for finding matches between two events differing in time. As per the results of the DTW process, letter 'i', 'j' no letter is displayed on the screen.</li><li>The sequence of letters obtained can be seen in the Raw text Field in the App. This "Raw Text" is stored in a String. </li><li>On pressing the Speak button, this data stored in the string is checked for spaces and grammatical errors, and appropriate corrections are made (as per American English) and the final String is displayed in the Formatted Text text box below the Speak button and the same is spoken too, which has been done using Text - To - Speech feature of  Android.</li></ul><p><strong>Problems faced:</strong></p><ul><li>Establishing UART connection between Bluetooth Module and the Micro-controller </li><li>Flex Sensor Calibration - As the flex sensors exhibit little hysteresis, so prolonged usage changes the readings. Thus we had to calibrate them more than once.</li></ul><p><strong>Further Extensions:</strong></p><ul>
<li>The same concept can be extended to words too. Using the three sensor readings, simple words involving one hand can be detected. Two gloves of the same kind will span the entire American Sign Language.</li><li>As of now we have used a Google Keyboard for Spell Check and space addition. This feature uses the same principle as the "Predictive Text feature" of the keyboard. Thus this system can be personalised by including user selective words in the dictionary that is accessed for Spell Check feature, thus making the app more user-friendly.</li><li>To remove the extensive wiring, <em>state-of-the-art </em>flexible conductive polymers can be used and he gloves can be stitched with the flex sensors starting from a piece of fabric. </li></ul><p><strong></strong></p>